{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "os.chdir(\"../\")\n",
    "from src.schemas.finetuning import FinetuningData\n",
    "from src.configs import (\n",
    "    SFTTrainerConfig, ModelConfig, TokenizerConfig,\n",
    "    LoRAConfig, TrainingArgs, BnBConfig, DatasetConfig, Push2HubConfig\n",
    ")\n",
    "from src.tuning.trainer import FineTuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "# print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flush():\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "  torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_dataset = load_dataset(\"mlabonne/guanaco-llama2-1k\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemma_prompt_format(example: str) -> str:\n",
    "    example[\"sentence1\"] = 'My sentence: ' + example[\"sentence1\"]\n",
    "    return example\n",
    "\n",
    "re.findall(re.escape('<s>')+\"(.*)\"+re.escape(\"</s>\"), \"<s>tgygcvytgivrzy</s>\")[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.max_memory_reserved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = FinetuningData(\n",
    "    llmodel_config=ModelConfig(pretrained_model_name_or_path=\"mistralai/Mistral-7B-Instruct-v0.1\", device_map=\"auto\"),\n",
    "    bnb_config=BnBConfig(),\n",
    "    dataset_config=DatasetConfig(name=\"konkinit/guanaco-llama2-100\"),\n",
    "    lora_config=LoRAConfig(),\n",
    "    sfttrainer_config=SFTTrainerConfig(),\n",
    "    tokenizer_config=TokenizerConfig(),\n",
    "    training_args=TrainingArgs(),\n",
    "    push2hub_config=Push2HubConfig(finetuned_model_name=\"Mistral-7B-Instruct-v0.1_finetuned\", commit_message=\"Llama-2-7b-chat-hf finetuned\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuner = FineTuner(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuner._train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.max_memory_allocated() / 1024 / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuner._push_to_HF_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
