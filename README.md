# LLM Finetuning

A project to get hands-on LLM finetuning by finetuning a Mistral model
on custom dataset I created.

## ToDo
- Document code
- Understand hyperparams

## Dataset



## Finetuning



## References

- [Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/sft_trainer)
- [Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes)
- [Reference Notebook](https://colab.research.google.com/drive/1Bd7c5rioBOmtJbDEax83vAHEPru-r06l#scrollTo=ib_We3NLtj2E)
- [GPU compatibility with format](https://www.reddit.com/r/MachineLearning/comments/vndtn8/d_mixed_precision_training_difference_between/?rdt=47864&onetap_auto=true&one_tap=true)
- [Finetuninbg with PEFT](https://huggingface.co/docs/peft/quicktour)
- [DataCamp Llama Finetuning](https://www.datacamp.com/tutorial/fine-tuning-llama-2)
